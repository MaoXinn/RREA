{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "from tqdm import *\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from CSLS import *\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.layers import *\n",
    "from layer import NR_GraphAttention\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True  \n",
    "sess = tf.Session(config=config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38960 3025\n"
     ]
    }
   ],
   "source": [
    "lang = 'zh'\n",
    "train_pair,dev_pair,adj_matrix,r_index,r_val,adj_features,rel_features = load_data('data/%s_en/'%lang,train_ratio=0.30)\n",
    "adj_matrix = np.stack(adj_matrix.nonzero(),axis = 1)\n",
    "rel_matrix,rel_val = np.stack(rel_features.nonzero(),axis = 1),rel_features.data\n",
    "ent_matrix,ent_val = np.stack(adj_features.nonzero(),axis = 1),adj_features.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_size = adj_features.shape[0]\n",
    "rel_size = rel_features.shape[1]\n",
    "triple_size = len(adj_matrix)\n",
    "batch_size = node_size\n",
    "\n",
    "\n",
    "class TokenEmbedding(keras.layers.Embedding):\n",
    "    \"\"\"Embedding layer with weights returned.\"\"\"\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.input_dim, self.output_dim\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.embeddings\n",
    "    \n",
    "def get_embedding():\n",
    "    inputs = [adj_matrix,r_index,r_val,rel_matrix,ent_matrix]\n",
    "    inputs = [np.expand_dims(item,axis=0) for item in inputs]\n",
    "    return get_emb.predict_on_batch(inputs)\n",
    "\n",
    "def test(wrank = None):\n",
    "    vec = get_embedding()\n",
    "    return  get_hits(vec,dev_pair,wrank=wrank)\n",
    "\n",
    "def CSLS_test(thread_number = 16, csls=10,accurate = True):\n",
    "    vec = get_embedding()\n",
    "    Lvec = np.array([vec[e1] for e1, e2 in dev_pair])\n",
    "    Rvec = np.array([vec[e2] for e1, e2 in dev_pair])\n",
    "    Lvec = Lvec / np.linalg.norm(Lvec,axis=-1,keepdims=True)\n",
    "    Rvec = Rvec / np.linalg.norm(Rvec,axis=-1,keepdims=True)\n",
    "    eval_alignment_by_sim_mat(Lvec, Rvec, [1, 5, 10], thread_number, csls=csls, accurate=accurate)\n",
    "    return None\n",
    "\n",
    "def get_train_set(batch_size = batch_size):\n",
    "    negative_ratio =  batch_size // len(train_pair) + 1\n",
    "    train_set = np.reshape(np.repeat(np.expand_dims(train_pair,axis=0),axis=0,repeats=negative_ratio),newshape=(-1,2))\n",
    "    np.random.shuffle(train_set); train_set = train_set[:batch_size]\n",
    "    train_set = np.concatenate([train_set,np.random.randint(0,node_size,train_set.shape)],axis = -1)\n",
    "    return train_set\n",
    "\n",
    "def get_trgat(node_size,rel_size,node_hidden,rel_hidden,triple_size,n_attn_heads = 2,dropout_rate = 0,gamma = 3,lr = 0.005,depth = 2):\n",
    "    adj_input = Input(shape=(None,2))\n",
    "    index_input = Input(shape=(None,2),dtype='int64')\n",
    "    val_input = Input(shape = (None,))\n",
    "    rel_adj = Input(shape=(None,2))\n",
    "    ent_adj = Input(shape=(None,2))\n",
    "    \n",
    "    ent_emb = TokenEmbedding(node_size,node_hidden,trainable = True)(val_input) \n",
    "    rel_emb = TokenEmbedding(rel_size,node_hidden,trainable = True)(val_input)\n",
    "    \n",
    "    def avg(tensor,size):\n",
    "        adj = K.cast(K.squeeze(tensor[0],axis = 0),dtype = \"int64\")   \n",
    "        adj = tf.SparseTensor(indices=adj, values=tf.ones_like(adj[:,0],dtype = 'float32'), dense_shape=(node_size,size)) \n",
    "        adj = tf.sparse_softmax(adj) \n",
    "        return tf.sparse_tensor_dense_matmul(adj,tensor[1])\n",
    "    \n",
    "    opt = [rel_emb,adj_input,index_input,val_input]\n",
    "    ent_feature = Lambda(avg,arguments={'size':node_size})([ent_adj,ent_emb])\n",
    "    rel_feature = Lambda(avg,arguments={'size':rel_size})([rel_adj,rel_emb])\n",
    "    \n",
    "    encoder = NR_GraphAttention(node_size,activation=\"relu\",\n",
    "                                       rel_size = rel_size,\n",
    "                                       depth = depth,\n",
    "                                       attn_heads=n_attn_heads,\n",
    "                                       triple_size = triple_size,\n",
    "                                       attn_heads_reduction='average',   \n",
    "                                       dropout_rate=dropout_rate)\n",
    "    \n",
    "    out_feature = Concatenate(-1)([encoder([ent_feature]+opt),encoder([rel_feature]+opt)])\n",
    "    out_feature = Dropout(dropout_rate)(out_feature)\n",
    "    \n",
    "    alignment_input = Input(shape=(None,4))\n",
    "    find = Lambda(lambda x:K.gather(reference=x[0],indices=K.cast(K.squeeze(x[1],axis=0), 'int32')))([out_feature,alignment_input])\n",
    "    \n",
    "    def align_loss(tensor):\n",
    "        def _cosine(x):\n",
    "            dot1 = K.batch_dot(x[0], x[1], axes=1)\n",
    "            dot2 = K.batch_dot(x[0], x[0], axes=1)\n",
    "            dot3 = K.batch_dot(x[1], x[1], axes=1)\n",
    "            max_ = K.maximum(K.sqrt(dot2 * dot3), K.epsilon())\n",
    "            return dot1 / max_\n",
    "        \n",
    "        def l1(ll,rr):\n",
    "            return K.sum(K.abs(ll-rr),axis=-1,keepdims=True)\n",
    "        \n",
    "        def l2(ll,rr):\n",
    "            return K.sum(K.square(ll-rr),axis=-1,keepdims=True)\n",
    "        \n",
    "        l,r,fl,fr = [tensor[:,0,:],tensor[:,1,:],tensor[:,2 ,:],tensor[:,3,:]]\n",
    "        loss = K.relu(gamma + l1(l,r) - l1(l,fr)) + K.relu(gamma + l1(l,r) - l1(fl,r))\n",
    "        return tf.reduce_sum(loss,keep_dims=True) / (batch_size)\n",
    "    \n",
    "    loss = Lambda(align_loss)(find)\n",
    "    \n",
    "    inputs = [adj_input,index_input,val_input,rel_adj,ent_adj]\n",
    "    train_model = keras.Model(inputs = inputs + [alignment_input],outputs = loss)\n",
    "    train_model.compile(loss=lambda y_true,y_pred: y_pred,optimizer=keras.optimizers.rmsprop(lr))\n",
    "    \n",
    "    feature_model = keras.Model(inputs = inputs,outputs = out_feature)\n",
    "    return train_model,feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_embedding_1 (TokenEmbeddi (38960, 100)         3896000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "token_embedding_2 (TokenEmbeddi (6050, 100)          605000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (38960, 100)         0           input_5[0][0]                    \n",
      "                                                                 token_embedding_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (38960, 100)         0           input_4[0][0]                    \n",
      "                                                                 token_embedding_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "nr__graph_attention_1 (NR_Graph (38960, 300)         600         lambda_1[0][0]                   \n",
      "                                                                 token_embedding_2[0][0]          \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 token_embedding_2[0][0]          \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (38960, 600)         0           nr__graph_attention_1[0][0]      \n",
      "                                                                 nr__graph_attention_1[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (38960, 600)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 4, 600)       0           dropout_1[0][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (1, 1)               0           lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,501,600\n",
      "Trainable params: 4,501,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model,get_emb = get_trgat(dropout_rate=0.30,node_size=node_size,rel_size=rel_size,n_attn_heads = 1,depth=2,gamma =3,node_hidden=100,rel_hidden = 100,triple_size = triple_size)\n",
    "model.summary(); initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 start.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 182/1200 [00:49<04:34,  3.71it/s]"
     ]
    }
   ],
   "source": [
    "rest_set_1 = [e1 for e1, e2 in dev_pair]\n",
    "rest_set_2 = [e2 for e1, e2 in dev_pair]\n",
    "np.random.shuffle(rest_set_1)\n",
    "np.random.shuffle(rest_set_2)\n",
    "\n",
    "epoch = 1200\n",
    "for turn in range(5):\n",
    "    print(\"iteration %d start.\"%turn)\n",
    "    for i in trange(epoch):\n",
    "        train_set = get_train_set()\n",
    "        inputs = [adj_matrix,r_index,r_val,rel_matrix,ent_matrix,train_set]\n",
    "        inputs = [np.expand_dims(item,axis=0) for item in inputs]\n",
    "        model.train_on_batch(inputs,np.zeros((1,1)))\n",
    "        if i%300 == 299:\n",
    "            CSLS_test()\n",
    "\n",
    "    new_pair = []\n",
    "    vec = get_embedding()\n",
    "    Lvec = np.array([vec[e] for e in rest_set_1])\n",
    "    Rvec = np.array([vec[e] for e in rest_set_2])\n",
    "    Lvec = Lvec / np.linalg.norm(Lvec,axis=-1,keepdims=True)\n",
    "    Rvec = Rvec / np.linalg.norm(Rvec,axis=-1,keepdims=True)\n",
    "    A,_ = eval_alignment_by_sim_mat(Lvec, Rvec, [1, 5, 10], 16,10,True,False)\n",
    "    B,_ = eval_alignment_by_sim_mat(Rvec, Lvec,[1, 5, 10], 16,10,True,False)\n",
    "    A = sorted(list(A)); B = sorted(list(B))\n",
    "    for a,b in A:\n",
    "        if  B[b][1] == a:\n",
    "            new_pair.append([rest_set_1[a],rest_set_2[b]])\n",
    "    print(\"generate new semi-pairs: %d.\" % len(new_pair))\n",
    "    \n",
    "    train_pair = np.concatenate([train_pair,np.array(new_pair)],axis = 0)\n",
    "    for e1,e2 in new_pair:\n",
    "        if e1 in rest_set_1:\n",
    "            rest_set_1.remove(e1) \n",
    "        \n",
    "    for e1,e2 in new_pair:\n",
    "        if e2 in rest_set_2:\n",
    "            rest_set_2.remove(e2) \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
